{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "## Configuration\n",
    "\n",
    "PYKILOSORT_DIR = os.environ.get('PYKILOSORT_DIR', None)\n",
    "if PYKILOSORT_DIR is None:\n",
    "    import pykilosort\n",
    "    PYKILOSORT_DIR = os.path.dirname(os.path.dirname(pykilosort.__file__))\n",
    "KILOSORT2_DIR = os.environ.get('KILOSORT2_DIR', f'{PYKILOSORT_DIR}/../Kilosort2')\n",
    "\n",
    "BASE_PATH = os.environ.get('BASE_PATH', f'{PYKILOSORT_DIR}/examples/eMouse/data')\n",
    "\n",
    "rootZ = '/home/alexmorley/git_repos/Kilosort2/datashift'\n",
    "rootH = '/tmp/'\n",
    "pathToYourConfigFile = '/home/alexmorley/git_repos/Kilosort2/configFiles'\n",
    "chanMapFile = 'NP2_kilosortChanMap.mat';\n",
    "\n",
    "opts = {\n",
    "    'chanMap': f'{rootZ}/{chanMapFile}',\n",
    "    'fs': 30000.,\n",
    "    'fshigh': 150.,\n",
    "    'minfr_goodchannels': 0.1000,\n",
    "    'Th': [6.0, 2.0],\n",
    "    'lam': 10.,\n",
    "    'AUCsplit': 0.9000,\n",
    "    'minFR': 0.0200,\n",
    "    'momentum': [20., 400],\n",
    "    'sigmaMask': 30.,\n",
    "    'ThPre': 8.,\n",
    "    'reorder': 1,\n",
    "    'nskip': 25.,\n",
    "    'spkTh': -6.,\n",
    "    'GPU': 1,\n",
    "    'nfilt_factor': 4.,\n",
    "    'ntbuff': 64.0,\n",
    "    'NT': 65600.,\n",
    "    'whiteningRange': 32.,\n",
    "    'nSkipCov': 25.0,\n",
    "    'scaleproc': 200.,\n",
    "    'nPCs': 3.,\n",
    "    'useRAM': 0,\n",
    "    'sorting': 2,\n",
    "    #'NchanTOT': float(simulation_opts['NchanTOT']),\n",
    "    'trange': [0., float('inf')],\n",
    "    'fproc': '/tmp/temp_wh.dat',\n",
    "    'rootZ': rootZ,\n",
    "    'fbinary': f'{rootZ}/data_cropped.bin',\n",
    "    'fig': False\n",
    "}\n",
    "\n",
    "opts= {\n",
    "    'chanMap': f'{rootZ}/{chanMapFile}',\n",
    "    'trange': [0., float('inf')],\n",
    "    'NchanTOT': float(384),\n",
    "    'minfr_goodchannels': 0.,\n",
    "    'sig': 20,\n",
    "    'fshigh': 300,\n",
    "    'trackfinal': 0,\n",
    "    'nblocks': 5,\n",
    "    'datashift': 1,\n",
    "    'fbinary': f'{rootZ}/data_cropped.bin',\n",
    "    'fs': 30000.,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from phylib.io.traces import get_ephys_reader\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from pykilosort.preprocess import preprocess, get_good_channels, get_whitening_matrix, get_Nbatch\n",
    "from pykilosort.cluster import clusterSingleBatches\n",
    "from pykilosort.learn import learnAndSolve8b\n",
    "from pykilosort.postprocess import find_merges, splitAllClusters, set_cutoff, rezToPhy\n",
    "from pykilosort.utils import Bunch, Context, memmap_large_array, load_probe\n",
    "from pykilosort.params import KilosortParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = pykilosort.Bunch()\n",
    "probe.NchanTOT = int(opts['NchanTOT'])\n",
    "probe.chanMap = np.load(f'{rootZ}/channel_map.npy').flatten().astype(int)\n",
    "probe.kcoords = np.ones(int(opts['NchanTOT']))\n",
    "probe.xc = np.load(rootZ+'/channel_positions.npy')[:,0]\n",
    "probe.yc = np.load(rootZ+'/channel_positions.npy')[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_path = opts['fbinary']\n",
    "dir_path = Path(rootZ)\n",
    "output_dir = Path(rootZ+'/py')\n",
    "probe = probe\n",
    "dtype = np.int16\n",
    "n_channels = int(opts['NchanTOT'])\n",
    "sample_rate = opts['fs']\n",
    "clear_context = False #Operations.pykilosort_sorting in FORCE_RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pykilosort.params.KilosortParams(**opts)\n",
    "raw_data = get_ephys_reader(dat_path, dtype=dtype, sample_rate=sample_rate, n_channels=n_channels)\n",
    "dir_path = dir_path or Path(dat_path).parent\n",
    "n_samples, n_channels = raw_data.shape\n",
    "logger.info(\"Loaded raw data with %d channels, %d samples.\", n_channels, n_samples)\n",
    "\n",
    "# Create the context.\n",
    "ctx_path = dir_path / \".kilosort\" / raw_data.name\n",
    "if clear_context:\n",
    "    logger.info(f\"Clearing context at {ctx_path} ...\")\n",
    "    shutil.rmtree(ctx_path, ignore_errors=True)\n",
    "ctx = Context(ctx_path)\n",
    "ctx.params = params\n",
    "ctx.probe = probe\n",
    "ctx.raw_data = raw_data    \n",
    "ctx.load()\n",
    "ir = ctx.intermediate\n",
    "ir.Nbatch = get_Nbatch(raw_data, params)\n",
    "probe.Nchan = probe.NchanTOT\n",
    "params.Nfilt = params.nfilt_factor * probe.Nchan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykilosort.preprocess import get_whitening_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(pykilosort.preprocess)\n",
    "from pykilosort.preprocess import get_whitening_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.Wrot = pykilosort.preprocess.get_whitening_matrix(\n",
    "    raw_data=raw_data, probe=probe, params=params\n",
    ")\n",
    "ctx.write(Wrot=ir.Wrot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.proc_path = ctx.path(\"proc\", \".dat\")\n",
    "preprocess(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.proc_path\n",
    "ir.proc = np.memmap(ir.proc_path, dtype=raw_data.dtype, mode=\"r\", order=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run datashift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.xc, ir.yc = probe.xc, probe.yc\n",
    "ir.ops = Bunch()\n",
    "\n",
    "# The min and max of the y and x ranges of the channels\n",
    "ymin = min(ir.yc)\n",
    "ymax = max(ir.yc)\n",
    "xmin = min(ir.xc)\n",
    "xmax = max(ir.xc)\n",
    "\n",
    "# Determine the average vertical spacing between channels. \n",
    "# Usually all the vertical spacings are the same, i.e. on Neuropixels probes. \n",
    "dmin = np.median(np.diff(np.unique(ir.yc)))\n",
    "print(f\"pitch is {dmin} um\\n\")\n",
    "ir.ops.yup = np.arange(start=ymin, step=dmin/2, stop=ymax) # centers of the upsampled y positions\n",
    "\n",
    "# Determine the template spacings along the x dimension\n",
    "x_range = xmax - xmin\n",
    "npt = math.floor(x_range/16) # this would come out as 16um for Neuropixels probes, which aligns with the geometry. \n",
    "ir.ops.xup = np.linspace(xmin, xmax, npt+1) # centers of the upsampled x positions\n",
    "\n",
    "spkTh = 10 # same as the usual \"template amplitude\", but for the generic templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.xc, ir.yc = probe.xc, probe.yc\n",
    "ir.ops = Bunch()\n",
    "\n",
    "# The min and max of the y and x ranges of the channels\n",
    "ymin = min(ir.yc)\n",
    "ymax = max(ir.yc)\n",
    "xmin = min(ir.xc)\n",
    "xmax = max(ir.xc)\n",
    "\n",
    "# Determine the average vertical spacing between channels. \n",
    "# Usually all the vertical spacings are the same, i.e. on Neuropixels probes. \n",
    "dmin = np.median(np.diff(np.unique(ir.yc)))\n",
    "print(f\"pitch is {dmin} um\\n\")\n",
    "ir.ops.yup = np.arange(start=ymin, step=dmin/2, stop=ymax) # centers of the upsampled y positions\n",
    "\n",
    "# Determine the template spacings along the x dimension\n",
    "x_range = xmax - xmin\n",
    "npt = math.floor(x_range/16) # this would come out as 16um for Neuropixels probes, which aligns with the geometry. \n",
    "ir.ops.xup = np.linspace(xmin, xmax, npt+1) # centers of the upsampled x positions\n",
    "\n",
    "spkTh = 10 # same as the usual \"template amplitude\", but for the generic templates\n",
    "\n",
    "# Extract all the spikes across the recording that are captured by the\n",
    "# generic templates. Very few real spikes are missed in this way. \n",
    "st3 = standalone_detector(ir, spkTh)\n",
    "\n",
    "# binning width across Y (um)\n",
    "dd = 5\n",
    "\n",
    "# detected depths\n",
    "dep = st3[:,2]\n",
    "\n",
    "# min and max for the range of depths\n",
    "dmin = ymin - 1\n",
    "dep = dep - dmin\n",
    "\n",
    "dmax  = 1 + ceil(max(dep)/dd)\n",
    "Nbatches      = ir.temp.Nbatch\n",
    "\n",
    "# which batch each spike is coming from\n",
    "batch_id = st3[:,5] #ceil[st3[:,1]/dt]\n",
    "\n",
    "# preallocate matrix of counts with 20 bins, spaced logarithmically\n",
    "F = zeros(dmax, 20, Nbatches)\n",
    "for t = 1:Nbatches\n",
    "    # find spikes in this batch\n",
    "    ix = np.where(batch_id==t)\n",
    "    \n",
    "    # subtract offset\n",
    "    dep = st3(ix,2) - dmin\n",
    "    \n",
    "    # amplitude bin relative to the minimum possible value\n",
    "    amp = log10(min(99, st3(ix,3))) - log10(spkTh)\n",
    "    \n",
    "    # normalization by maximum possible value\n",
    "    amp = amp / (log10(100) - log10(spkTh))\n",
    "    \n",
    "    # multiply by 20 to distribute a [0,1] variable into 20 bins\n",
    "    # sparse is very useful here to do this binning quickly\n",
    "    M = sparse(ceil(dep/dd), ceil(1e-5 + amp * 20), ones(numel(ix), 1), dmax, 20)    \n",
    "    \n",
    "    # the counts themselves are taken on a logarithmic scale (some neurons\n",
    "    # fire too much!)\n",
    "    F[:, :, t] = log2(1+M)\n",
    "end\n",
    "\n",
    "##\n",
    "# the 'midpoint' branch is for chronic recordings that have been\n",
    "# concatenated in the binary file\n",
    "#if isfield(ops, 'midpoint')\n",
    "#    # register the first block as usual\n",
    "#    [imin1, F1] = align_block(F(:, :, 1:ops.midpoint))\n",
    "#    # register the second block as usual\n",
    "#    [imin2, F2] = align_block(F(:, :, ops.midpoint+1:end))\n",
    "#    # now register the average first block to the average second block\n",
    "#    d0 = align_pairs(F1, F2)\n",
    "#    # concatenate the shifts\n",
    "#    imin = [imin1 imin2 + d0]\n",
    "#    imin = imin - mean(imin)\n",
    "#    ops.datashift = 1\n",
    "#else\n",
    "#    # determine registration offsets \n",
    "#    ysamp = dmin + dd * [1:dmax] - dd/2\n",
    "#    [imin,yblk, F0] = align_block2(F, ysamp, ops.nblocks)\n",
    "#end\n",
    "\n",
    "##\n",
    "if opts.get('fig', True):  \n",
    "    ax = plt.subplot()\n",
    "    # plot the shift trace in um\n",
    "    ax.plot(imin * dd)\n",
    "    \n",
    "    ax = plt.subplot()\n",
    "    # raster plot of all spikes at their original depths\n",
    "    st_shift = st3(:,2) #+ imin(batch_id)' * dd\n",
    "    for j = spkTh:100\n",
    "        # for each amplitude bin, plot all the spikes of that size in the\n",
    "        # same shade of gray\n",
    "        ix = st3(:, 3)==j # the amplitudes are rounded to integers\n",
    "        ax.plot(st3(ix, 1), st_shift(ix), '.', 'color', [1 1 1] * max(0, 1-j/40)) # the marker color here has been carefully tuned\n",
    "    plt.tight_layout()\n",
    "\n",
    "# if we're creating a registered binary file for visualization in Phy\n",
    "if opts.get('fbinaryproc', False):\n",
    "    with open(opts['fbinaryproc'], 'w') as f:\n",
    "        pass\n",
    "\n",
    "# convert to um \n",
    "dshift = imin * dd\n",
    "# sort in case we still want to do \"tracking\"\n",
    "\n",
    "_, ir.iorig = np.sort(np.mean(dshift, 2))\n",
    "\n",
    "# sigma for the Gaussian process smoothing\n",
    "sig = ir.ops.sig\n",
    "# register the data batch by batch\n",
    "for ibatch in range(Nbatches):\n",
    "    shift_batch_on_disk2(ir, ibatch, dshift[ibatch, :], yblk, sig)\n",
    "end\n",
    "fprintf('time #2.2f, Shifted up/down #d batches. \\n', toc, Nbatches)\n",
    "\n",
    "# keep track of dshift \n",
    "ir.dshift = dshift\n",
    "# keep track of original spikes\n",
    "ir.st0 = st3\n",
    "\n",
    "\n",
    "# next, we can just run a normal spike sorter, like Kilosort1, and forget about the transformation that has happened in here \n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbatch = ctx.intermediate.Nbatch\n",
    "proc = ir.proc\n",
    "\n",
    "NT = params.NT\n",
    "nPCs = params.nPCs\n",
    "Nchan = probe.Nchan\n",
    "\n",
    "batchstart = np.arange(0, NT * Nbatch + 1, NT).astype(np.int64)\n",
    "\n",
    "# extract the PCA projections\n",
    "# initialize the covariance of single-channel spike waveforms\n",
    "CC = cp.zeros(params.nt0, dtype=np.float32)\n",
    "\n",
    "# from every 100th batch\n",
    "for ibatch in range(0, Nbatch, 100):\n",
    "    offset = Nchan * batchstart[ibatch]\n",
    "    dat = proc.flat[offset:offset + NT * Nchan].reshape((-1, Nchan),\n",
    "                                                        order='F')\n",
    "    if dat.shape[0] == 0:\n",
    "        continue\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"st3\" not in ir:\n",
    "    with ctx.time(\"learn\"):\n",
    "        out = learnAndSolve8b(ctx)\n",
    "    logger.info(\"%d spikes.\", ir.st3.shape[0])\n",
    "    ctx.save(**out)\n",
    "if stop_after == \"learn\":\n",
    "    return ctx\n",
    "# Special care for cProj and cProjPC which are memmapped .dat files.\n",
    "ir.cProj = memmap_large_array(ctx.path(\"fW\", ext=\".dat\")).T\n",
    "ir.cProjPC = memmap_large_array(ctx.path(\"fWpc\", ext=\".dat\")).T  # transpose\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Final merges.\n",
    "#\n",
    "# This function uses:\n",
    "#\n",
    "#       st3, simScore\n",
    "#\n",
    "# This function saves:\n",
    "#\n",
    "#         st3_m,\n",
    "#         R_CCG, Q_CCG, K_CCG [optional]\n",
    "#\n",
    "if \"st3_m\" not in ir:\n",
    "    with ctx.time(\"merge\"):\n",
    "        out = find_merges(ctx, True)\n",
    "    ctx.save(**out)\n",
    "if stop_after == \"merge\":\n",
    "    return ctx\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Final splits.\n",
    "#\n",
    "# This function uses:\n",
    "#\n",
    "#       st3_m\n",
    "#       W, dWU, cProjPC,\n",
    "#       iNeigh, simScore\n",
    "#       wPCA\n",
    "#\n",
    "# This function saves:\n",
    "#\n",
    "#       st3_s\n",
    "#       W_s, U_s, mu_s, simScore_s\n",
    "#       iNeigh_s, iNeighPC_s,\n",
    "#       Wphy, iList, isplit\n",
    "#\n",
    "if \"st3_s1\" not in ir:\n",
    "    # final splits by SVD\n",
    "    with ctx.time(\"split_1\"):\n",
    "        out = splitAllClusters(ctx, True)\n",
    "    # Use a different name for both splitting steps.\n",
    "    out[\"st3_s1\"] = out.pop(\"st3_s\")\n",
    "    ctx.save(**out)\n",
    "if stop_after == \"split_1\":\n",
    "    return ctx\n",
    "\n",
    "if \"st3_s0\" not in ir:\n",
    "    # final splits by amplitudes\n",
    "    with ctx.time(\"split_2\"):\n",
    "        out = splitAllClusters(ctx, False)\n",
    "    out[\"st3_s0\"] = out.pop(\"st3_s\")\n",
    "    ctx.save(**out)\n",
    "if stop_after == \"split_2\":\n",
    "    return ctx\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Decide on cutoff.\n",
    "#\n",
    "# This function uses:\n",
    "#\n",
    "#       st3_s\n",
    "#       dWU, cProj, cProjPC\n",
    "#       wPCA\n",
    "#\n",
    "# This function saves:\n",
    "#\n",
    "#       st3_c, spikes_to_remove,\n",
    "#       est_contam_rate, Ths, good\n",
    "#\n",
    "if \"st3_c\" not in ir:\n",
    "    with ctx.time(\"cutoff\"):\n",
    "        out = set_cutoff(ctx)\n",
    "    ctx.save(**out)\n",
    "if stop_after == \"cutoff\":\n",
    "    return ctx\n",
    "\n",
    "logger.info(\"%d spikes after cutoff.\", ir.st3_c.shape[0])\n",
    "logger.info(\"Found %d good units.\", np.sum(ir.good > 0))\n",
    "\n",
    "# write to Phy\n",
    "logger.info(\"Saving results to phy.\")\n",
    "output_dir = output_dir or f\"{dir_path}/output\"\n",
    "with ctx.time(\"output\"):\n",
    "    rezToPhy(ctx, dat_path=dat_path, output_dir=output_dir)\n",
    "\n",
    "# Show timing information.\n",
    "ctx.show_timer()\n",
    "ctx.write(timer=ctx.timer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
